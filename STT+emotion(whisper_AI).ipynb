{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/coderTanisha22/Jarvis_learnings/blob/main/STT%2Bemotion(whisper_AI).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXal4CcCKjB1"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git #installing whisper ai from gitub, pip clones\n",
        "!pip install -q transformers torchaudio soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvTbeU5qKkjl"
      },
      "outputs": [],
      "source": [
        "from google.colab import files #used in google colab to interact with the device\n",
        "\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "print(\"Uploaded file:\", audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQWa_Y1qK8ic"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "\n",
        "# Load whisper model (base for speed, use 'medium' or 'large' for better accuracy)\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(audio_file)\n",
        "\n",
        "#transcription output\n",
        "print(\"Transcription:\\n\", result['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWVc30v5L0zM"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rASb4SiLRIL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torchaudio\n",
        "import torch\n",
        "\n",
        "extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "emo_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\")\n",
        "emo_model.eval()\n",
        "\n",
        "#Load and preprocess audio\n",
        "waveform, sr = torchaudio.load(audio_file)\n",
        "if sr != 16000:\n",
        "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n",
        "    waveform = resampler(waveform)\n",
        "waveform = waveform.squeeze()\n",
        "\n",
        "#Get prediction\n",
        "inputs = extractor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    logits = emo_model(**inputs).logits\n",
        "\n",
        "pred_id = int(torch.argmax(logits))\n",
        "emotion = emo_model.config.id2label[pred_id]\n",
        "\n",
        "print(\"Detected Emotion:\", emotion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7CPP1MIqohB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5e7WG9ivH1jcSzd5aRM0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}